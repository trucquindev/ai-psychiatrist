import json
from pymongo import MongoClient
from langchain_core.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

load_dotenv()

# K·∫øt n·ªëi MongoDB
MONGO_URI = os.getenv('MONGO_URI')
client = MongoClient(MONGO_URI)
db = client["chatbot_db"]
chat_collection = db["chats"]

# T·∫£i k·ªãch b·∫£n c√¢u h·ªèisdsdj
def load_question_templates():
    with open("question_templates.json", "r", encoding="utf-8") as f:
        return json.load(f)

question_templates = load_question_templates()

# K·∫øt n·ªëi Ollama AI
llm = Ollama(base_url="http://localhost:11434", model="qwen2.5-coder:0.5b")

template = PromptTemplate.from_template(
    """
    D∆∞·ªõi ƒë√¢y l√† cu·ªôc h·ªôi tho·∫°i c·ªßa m·ªôt b·ªánh nh√¢n v·ªõi b√°c sƒ© t√¢m l√Ω:

    {context}
    
    {context_db}
    
    Tr·∫£ l·ªùi c√¢u h·ªèi sau:
    {question}

    Ph·∫£n h·ªìi c·ªßa b√°c sƒ©:
    """
)

llm_chain = (
  template
  | llm
  | StrOutputParser()
)

def handle_chat_normal(user_message,user_id,normal):
    user_message = user_message.lower()
    chat_session_normal= chat_collection.find_one({"user_id": user_id, "normal": True}, sort=[("_id", -1)])
    print("DEBUG chat_session_normal:", chat_session_normal)
    # chat_session_normal = chat_collection.find_one({"user_id": user_id})
    # print("DEBUG result:", chat_session_normal)

    if chat_session_normal:
        answers = chat_session_normal["answers"]
        bot= chat_session_normal["bot"]
        answers.append(user_message)
        # update_data = {"answers": answers, "step": step}
        response = llm_chain.invoke({"context": "", "context_db": "", "question": user_message})
        print("DEBUG response:", response)
        bot.append(response)
        chat_collection.update_one({"_id": chat_session_normal["_id"]}, {"$set": {"answers": answers, "bot": bot}})
        return {"response": response, "normal": normal}
    return {"normal":normal, "response": "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu."}
# üîπ H√†m x·ª≠ l√Ω khi ng∆∞·ªùi d√πng nh·∫≠p tin nh·∫Øn ban ƒë·∫ßu
def process_message(user_message, user_id):
    user_message = user_message.lower()

    # Ki·ªÉm tra xem user c√≥ ƒëang trong cu·ªôc h·ªôi tho·∫°i kh√¥ng
    chat_session = chat_collection.find_one({"user_id": user_id, "completed": False}, sort=[("_id", -1)])

    if chat_session:
        return handle_answer(user_message, user_id)

    # N·∫øu user b·∫Øt ƒë·∫ßu m·ªôt h·ªôi tho·∫°i m·ªõi
    for key in question_templates.keys():
        if key in user_message:
            questions = question_templates[key]

            chat_collection.insert_one({
                "user_id": user_id,
                "user": user_message,
                "bot": "T√¥i s·∫Ω h·ªèi b·∫°n m·ªôt s·ªë c√¢u ƒë·ªÉ hi·ªÉu r√µ h∆°n.",
                "context": key,
                "questions": questions,
                "answers": [],
                "step": 0,  # B·∫Øt ƒë·∫ßu t·ª´ c√¢u h·ªèi ƒë·∫ßu ti√™n
                "completed": False
            })

            return {"response": questions[0], "context": key}
    
    # N·∫øu kh√¥ng t√¨m th·∫•y c√¢u h·ªèi n√†o trong templates, g·ª≠i ƒë·∫øn AI ƒë·ªÉ x·ª≠ l√Ω
    response = llm_chain.invoke({"context": "", "context_db": "", "question": user_message})
    return {"response": response,"normal": True}
# üîπ H√†m x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng
def handle_answer(user_message, user_id):
    chat_session = chat_collection.find_one({"user_id": user_id, "completed": False}, sort=[("_id", -1)])
    if chat_session:
        questions = chat_session["questions"]
        answers = chat_session["answers"]
        step = chat_session["step"]

        answers.append(user_message)
        step += 1

        update_data = {"answers": answers, "step": step}

        if step < len(questions):  # C√≤n c√¢u h·ªèi ti·∫øp theo
            update_data["completed"] = False
            chat_collection.update_one({"_id": chat_session["_id"]}, {"$set": update_data})
            return {"response": {"response":questions[step]}}

        else:  # H·∫øt c√¢u h·ªèi, ti·∫øn h√†nh ph√¢n t√≠ch
            update_data["completed"] = True
            chat_collection.update_one({"_id": chat_session["_id"]}, {"$set": update_data})
            analysis = analyze_answers(chat_session["context"], answers)
            return {"response": analysis}

   
    return {"response":{"response":"Xin l·ªói, t√¥i kh√¥ng hi·ªÉu."}}

# üîπ H√†m g·ª≠i d·ªØ li·ªáu ƒë·∫øn AI ƒë·ªÉ ph√¢n t√≠ch khi k·∫øt th√∫c c√¢u h·ªèi
def analyze_answers(context, answers):
    # T·∫°o prompt g·ª≠i ƒë·∫øn AI
    prompt = f"T√¥i ƒëang t∆∞ v·∫•n cho m·ªôt b·ªánh nh√¢n v·ªÅ v·∫•n ƒë·ªÅ {context}. H·ªç ƒë√£ tr·∫£ l·ªùi:\n"
    for idx, ans in enumerate(answers):
        prompt += f"C√¢u {idx+1}: {ans}\n"
    prompt += "B·∫°n c√≥ th·ªÉ ph√¢n t√≠ch t√¨nh tr·∫°ng c·ªßa h·ªç v√† ƒë∆∞a ra l·ªùi khuy√™n kh√¥ng?"
    # üî• G·ª≠i ƒë·∫øn AI ƒë·ªÉ ph√¢n t√≠ch
    analysis = llm_chain.invoke({"context": "T∆∞ v·∫•n t√¢m l√Ω", "context_db": "", "question": prompt})
    # Tr·∫£ v·ªÅ format ƒë√∫ng
    return {"response":analysis }
   
